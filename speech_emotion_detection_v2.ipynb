{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8fcdf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "from architectures.TimeCNN import TimeCNN\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b88e88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAVDESS\n",
    "\n",
    "path = 'datasets/archive/audio_speech_actors_01-24'\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "val_data = []\n",
    "\n",
    "# Speaker split (gender-balanced)\n",
    "train_speakers = [\"01\",\"03\",\"05\",\"07\",\"09\",\"11\",\"13\",\"15\",\"17\",\"02\",\"04\",\"06\",\"08\",\"10\",\"12\",\"14\",\"16\"]\n",
    "val_speakers = [\"19\",\"20\",\"18\"]  # added validation split, took speakers form train split\n",
    "test_speakers = [\"21\",\"23\",\"22\",\"24\"]\n",
    "\n",
    "emotion_map = {\n",
    "    \"01\": \"neutral\",\n",
    "    # \"02\": \"calm\",  -> remove not in crema-d\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    # \"08\": \"surprised\" -> remove not in crema-d\n",
    "}\n",
    "\n",
    "for folder in sorted(os.listdir(path)):\n",
    "    folder_path = os.path.join(path, folder)\n",
    "    for file in os.listdir(folder_path):\n",
    "        match = re.match(r\"(\\d+)-(\\d+)-(\\d+)-(\\d+)-(\\d+)-(\\d+)-(\\d+)\\.wav\", file)\n",
    "        if match:\n",
    "            filepath = str(Path(path) / folder / file)\n",
    "            indicators = match.groups()\n",
    "            emotion_code = indicators[2]\n",
    "            actor_id = indicators[-1]\n",
    "            gender = 'Female' if int(indicators[-1]) % 2 == 0 else 'Male'\n",
    "            if emotion_code not in emotion_map:\n",
    "                continue\n",
    "\n",
    "            record = {\n",
    "                \"path\": filepath,\n",
    "                \"emotion\": emotion_map[emotion_code],\n",
    "                \"gender\": gender,\n",
    "                \"source\": \"RAVDESS\"\n",
    "            }\n",
    "\n",
    "            if actor_id in train_speakers:\n",
    "                train_data.append(record)\n",
    "            elif actor_id in val_speakers:\n",
    "                val_data.append(record)\n",
    "            else:\n",
    "                test_data.append(record)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "val_df = pd.DataFrame(val_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "train_df.to_csv(\"train_split.csv\", index=False)\n",
    "val_df.to_csv(\"val_split.csv\", index=False)\n",
    "test_df.to_csv(\"test_split.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1148a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(748, 4)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9991b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ActorID  Age     Sex              Race     Ethnicity\n",
      "0     1001   51    Male         Caucasian  Not Hispanic\n",
      "1     1002   21  Female         Caucasian  Not Hispanic\n",
      "2     1003   21  Female         Caucasian  Not Hispanic\n",
      "3     1004   42  Female         Caucasian  Not Hispanic\n",
      "4     1005   29    Male  African American  Not Hispanic\n",
      "5     1006   58  Female         Caucasian  Not Hispanic\n",
      "6     1007   38  Female  African American  Not Hispanic\n",
      "7     1008   46  Female         Caucasian  Not Hispanic\n",
      "8     1009   24  Female         Caucasian  Not Hispanic\n",
      "9     1010   27  Female         Caucasian  Not Hispanic\n",
      "Index(['ActorID', 'Age', 'Sex', 'Race', 'Ethnicity'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "demo_df = pd.read_csv('datasets/CREMA-D/VideoDemographics.csv')\n",
    "print(demo_df.head(10))\n",
    "print(demo_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ea612c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total actors: 91\n",
      "Males: 48\n",
      "Females: 43\n"
     ]
    }
   ],
   "source": [
    "# CREMA-D\n",
    "\n",
    "path = 'datasets/CREMA-D/AudioWAV'\n",
    "\n",
    "# Load demographics for gender mapping\n",
    "demo_df = pd.read_csv('datasets/CREMA-D/VideoDemographics.csv')\n",
    "gender_map = dict(zip(demo_df['ActorID'], demo_df['Sex']))\n",
    "\n",
    "# Check how many actors\n",
    "print(f\"Total actors: {len(gender_map)}\")\n",
    "print(f\"Males: {list(gender_map.values()).count('Male')}\")\n",
    "print(f\"Females: {list(gender_map.values()).count('Female')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "711d485c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train speakers: 68 (34M, 34F)\n",
      "Validation speakers: 8 (4M, 4F)\n",
      "Test speakers: 15 (10M, 5F)\n",
      "\n",
      "CREMA-D TRAIN:\n",
      "  Samples: 5557\n",
      "  By gender: {'Male': 2782, 'Female': 2775}\n",
      "\n",
      "CREMA-D VALIDATION:\n",
      "  Samples: 655\n",
      " by gender: {'Male': 328, 'Female': 327}\n",
      "\n",
      "CREMA-D TEST:\n",
      "  Samples: 1230\n",
      "  By gender: {'Male': 820, 'Female': 410}\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "val_data = []\n",
    "\n",
    "emotion_map = {\n",
    "    \"ANG\": \"angry\",\n",
    "    \"DIS\": \"disgust\",\n",
    "    \"FEA\": \"fearful\",\n",
    "    \"HAP\": \"happy\",\n",
    "    \"NEU\": \"neutral\",\n",
    "    \"SAD\": \"sad\"\n",
    "}\n",
    "\n",
    "# Get all actor IDs and split by gender\n",
    "all_actors = demo_df['ActorID'].tolist()\n",
    "male_actors = demo_df[demo_df['Sex'] == 'Male']['ActorID'].tolist()\n",
    "female_actors = demo_df[demo_df['Sex'] == 'Female']['ActorID'].tolist()\n",
    "\n",
    "# 80/20 split per gender\n",
    "train_males = male_actors[:34]      # 34 males for train\n",
    "val_males = male_actors[34:38]      # 4 males for validation\n",
    "test_males = male_actors[38:]       # 10 males for test\n",
    "\n",
    "train_females = female_actors[:34]  # 34 females for train  \n",
    "val_females = female_actors[34:38]  # 4 females for validation\n",
    "test_females = female_actors[38:]   # 5 females for test\n",
    "\n",
    "train_speakers = train_males + train_females  # 72 speakers\n",
    "val_speakers = val_males + val_females        # 8 speakers\n",
    "test_speakers = test_males + test_females      # 19 speakers\n",
    "\n",
    "print(f\"\\nTrain speakers: {len(train_speakers)} ({len(train_males)}M, {len(train_females)}F)\")\n",
    "print(f\"Validation speakers: {len(val_speakers)} ({len(val_males)}M, {len(val_females)}F)\")\n",
    "print(f\"Test speakers: {len(test_speakers)} ({len(test_males)}M, {len(test_females)}F)\")\n",
    "\n",
    "# Parse audio files\n",
    "for file in os.listdir(path):\n",
    "    if not file.endswith('.wav'):\n",
    "        continue\n",
    "        \n",
    "    # Filename: 1001_IEO_ANG_HI.wav\n",
    "    parts = file.replace('.wav', '').split('_')\n",
    "    \n",
    "    if len(parts) < 4:\n",
    "        continue\n",
    "    \n",
    "    actor_id = int(parts[0])\n",
    "    emotion_code = parts[2]\n",
    "    \n",
    "    if emotion_code not in emotion_map:\n",
    "        continue\n",
    "    \n",
    "    filepath = str(Path(path) / file)\n",
    "    gender = gender_map.get(actor_id, 'Unknown')\n",
    "    \n",
    "    record = {\n",
    "        \"path\": filepath,\n",
    "        \"emotion\": emotion_map[emotion_code],\n",
    "        \"gender\": gender,\n",
    "        \"source\": \"CREMA-D\"\n",
    "    }\n",
    "    \n",
    "    if actor_id in train_speakers:\n",
    "        train_data.append(record)\n",
    "    elif actor_id in val_speakers:\n",
    "        val_data.append(record)\n",
    "    elif actor_id in test_speakers:\n",
    "        test_data.append(record)\n",
    "\n",
    "# Create DataFrames\n",
    "train_df_cremad = pd.DataFrame(train_data)\n",
    "val_df_cremad = pd.DataFrame(val_data)\n",
    "test_df_cremad = pd.DataFrame(test_data)\n",
    "\n",
    "# Verify\n",
    "print(\"\\nCREMA-D TRAIN:\")\n",
    "print(f\"  Samples: {len(train_df_cremad)}\")\n",
    "print(f\"  By gender: {train_df_cremad['gender'].value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\nCREMA-D VALIDATION:\")\n",
    "print(f\"  Samples: {len(val_df_cremad)}\")\n",
    "print(f\" by gender: {val_df_cremad['gender'].value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\nCREMA-D TEST:\")\n",
    "print(f\"  Samples: {len(test_df_cremad)}\")\n",
    "print(f\"  By gender: {test_df_cremad['gender'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b6266a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL TRAIN: 6305\n",
      "FINAL VALIDATION: 787\n",
      "FINAL TEST: 1406\n"
     ]
    }
   ],
   "source": [
    "train_df_combined = pd.concat([train_df, train_df_cremad], ignore_index=True)\n",
    "val_df_combined = pd.concat([val_df, val_df_cremad], ignore_index=True)\n",
    "test_df_combined = pd.concat([test_df, test_df_cremad], ignore_index=True)\n",
    "\n",
    "train_df_combined.to_csv(\"train_split.csv\", index=False)\n",
    "val_df_combined.to_csv(\"val_split.csv\", index=False)\n",
    "test_df_combined.to_csv(\"test_split.csv\", index=False)\n",
    "\n",
    "print(f\"\\nFINAL TRAIN: {len(train_df_combined)}\")\n",
    "print(f\"FINAL VALIDATION: {len(val_df_combined)}\")\n",
    "print(f\"FINAL TEST: {len(test_df_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "469d1a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'happy' 'sad' 'angry' 'fearful' 'disgust']\n",
      "['neutral' 'happy' 'sad' 'angry' 'fearful' 'disgust']\n",
      "['neutral' 'happy' 'sad' 'angry' 'fearful' 'disgust']\n"
     ]
    }
   ],
   "source": [
    "# quick check to ensure \"calm\" emotion is removed\n",
    "print(train_df['emotion'].unique())\n",
    "print(val_df['emotion'].unique())\n",
    "print(test_df['emotion'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16a3f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_path = train_df.iloc[0]['path']\n",
    "# sample_emotion = train_df.iloc[0]['emotion']\n",
    "# print (f\"Sample path: {sample_path},\\nemotion: {sample_emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32f92c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sample_rate_1 = 16000\n",
    "# sample_rate_2 = 22050\n",
    "# duration_1 = 3\n",
    "# duration_2 = 5\n",
    "\n",
    "\n",
    "# waveform_1, sr_1 = librosa.load(sample_path, sr=sample_rate_1)\n",
    "# print(f\"\\nOriginal_1:\")\n",
    "# print(f\"  Sample rate: {sr_1} Hz\")\n",
    "# print(f\"  Shape: {waveform_1.shape}\")\n",
    "# print(f\"  Duration: {len(waveform_1)/sr_1:.2f} seconds\")\n",
    "\n",
    "# waveform_2, sr_2 = librosa.load(sample_path, sr=sample_rate_2)\n",
    "# print(f\"\\nOriginal_2:\")\n",
    "# print(f\"  Sample rate: {sr_2} Hz\")\n",
    "# print(f\"  Shape: {waveform_2.shape}\")\n",
    "# print(f\"  Duration: {len(waveform_2)/sr_2:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48c9eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_length_1 = sample_rate_1 * duration_1  # 3 seconds at 16kHz\n",
    "# target_length_2 = sample_rate_2 * duration_2  # 5 seconds at 22.05kHz\n",
    "\n",
    "# waveform_1 = librosa.util.fix_length(waveform_1, size=target_length_1)\n",
    "# waveform_2 = librosa.util.fix_length(waveform_2, size=target_length_2)\n",
    "\n",
    "# print(f\"Final waveform_1 shape: {waveform_1.shape}\")\n",
    "# print(f\"Final waveform_2 shape: {waveform_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7709bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mel_spec_1 = librosa.feature.melspectrogram(\n",
    "#     y=waveform_1,\n",
    "#     sr=sample_rate_1,\n",
    "#     n_mels = 128,\n",
    "#     n_fft=2048,\n",
    "#     hop_length=512\n",
    "# )\n",
    "# print(f\"\\nMel-spectrogram shape: {mel_spec_1.shape}\")\n",
    "\n",
    "# mel_spec_2 = librosa.feature.melspectrogram(\n",
    "#     y=waveform_2,\n",
    "#     sr=sample_rate_2,\n",
    "#     n_mels = 128,\n",
    "#     n_fft=2048,\n",
    "#     hop_length=512\n",
    "# )\n",
    "# print(f\"\\nMel-spectrogram shape: {mel_spec_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "090be072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mel_spec_db_1 = librosa.power_to_db(mel_spec_1, ref=np.max)\n",
    "# print(f\"Mel-spectrogram (dB) shape: {mel_spec_db_1.shape}\")\n",
    "# mel_spec_db_2 = librosa.power_to_db(mel_spec_2, ref=np.max)\n",
    "# print(f\"Mel-spectrogram (dB) shape: {mel_spec_db_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e753fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============== PLOT COMPARISON ==============\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# # Row 1: Waveforms\n",
    "# axes[0, 0].plot(waveform_1)\n",
    "# axes[0, 0].set_title(f'Waveform - 16kHz, 3 sec\\nSamples: {len(waveform_1):,}')\n",
    "# axes[0, 0].set_xlabel('Samples')\n",
    "# axes[0, 0].set_ylabel('Amplitude')\n",
    "\n",
    "# axes[0, 1].plot(waveform_2)\n",
    "# axes[0, 1].set_title(f'Waveform - 22kHz, 5 sec\\nSamples: {len(waveform_2):,}')\n",
    "# axes[0, 1].set_xlabel('Samples')\n",
    "# axes[0, 1].set_ylabel('Amplitude')\n",
    "\n",
    "# # Row 2: Mel-Spectrograms (dB)\n",
    "# img1 = librosa.display.specshow(\n",
    "#     mel_spec_db_1, x_axis='time', y_axis='mel', sr=sample_rate_1, ax=axes[1, 0]\n",
    "# )\n",
    "# axes[1, 0].set_title(f'Mel-Spectrogram (YOUR SETTINGS)\\nShape: {mel_spec_db_1.shape}')\n",
    "# fig.colorbar(img1, ax=axes[1, 0], format='%+2.0f dB')\n",
    "\n",
    "# img2 = librosa.display.specshow(\n",
    "#     mel_spec_db_2, x_axis='time', y_axis='mel', sr=sample_rate_2, ax=axes[1, 1]\n",
    "# )\n",
    "# axes[1, 1].set_title(f'Mel-Spectrogram (REFERENCE SETTINGS)\\nShape: {mel_spec_db_2.shape}')\n",
    "# fig.colorbar(img2, ax=axes[1, 1], format='%+2.0f dB')\n",
    "\n",
    "# plt.suptitle(f'Comparison: {sample_emotion.upper()}', fontsize=14, fontweight='bold')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('audio_comparison.png', dpi=150)\n",
    "# plt.show()\n",
    "\n",
    "# # ============== SUMMARY ==============\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# print(\"COMPARISON SUMMARY\")\n",
    "# print(\"=\" * 60)\n",
    "# print(f\"\\n{'Setting':<25} {'Yours':<20} {'Reference':<20}\")\n",
    "# print(\"-\" * 60)\n",
    "# print(f\"{'Sample Rate':<25} {sample_rate_1:,} Hz{'':<10} {sample_rate_2:,} Hz\")\n",
    "# print(f\"{'Duration':<25} {duration_1} sec{'':<15} {duration_2} sec\")\n",
    "# print(f\"{'Total Samples':<25} {len(waveform_1):,}{'':<13} {len(waveform_2):,}\")\n",
    "# print(f\"{'Spectrogram Shape':<25} {mel_spec_db_1.shape}{'':<11} {mel_spec_db_2.shape}\")\n",
    "# print(f\"{'Time Frames':<25} {mel_spec_db_1.shape[1]}{'':<17} {mel_spec_db_2.shape[1]}\")\n",
    "# print(\"-\" * 60)\n",
    "# print(f\"\\nReference has {mel_spec_db_2.shape[1] / mel_spec_db_1.shape[1]:.1f}x more temporal information!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16faa9",
   "metadata": {},
   "source": [
    "128 mel bands - frequency range leading up to 48000hz (3 sec time)\n",
    "\n",
    "94 time windows\n",
    "\n",
    "hence the shape (128, 94)\n",
    "\n",
    "notice the power db graph (scale adjusted for human audible samples) - we set the loudest volume (highest amplitude) to 0, closest to that will be loude voice represented with bright colors and then leading away from it is quiter around -80 db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80800e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_time_features(waveform, sr):\n",
    "#     # Zero Crossing Rate\n",
    "#     zcr = librosa.feature.zero_crossing_rate(\n",
    "#         waveform, frame_length=2048, hop_length=512\n",
    "#     )\n",
    "    \n",
    "#     # RMS Energy\n",
    "#     energy = librosa.feature.rms(\n",
    "#         y=waveform, frame_length=2048, hop_length=512\n",
    "#     )\n",
    "    \n",
    "#     # MFCCs\n",
    "#     mfccs = librosa.feature.mfcc(\n",
    "#         y=waveform, sr=sr, n_fft=2048, hop_length=512, n_mfcc=13\n",
    "#     )\n",
    "    \n",
    "#     return zcr, energy, mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb67c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# angry_sample  = train_df[train_df['emotion'] == 'angry'].iloc[0]\n",
    "# sad_sample = train_df[train_df['emotion'] == 'sad'].iloc[0]\n",
    "\n",
    "# sample_rate = 22050\n",
    "# duration = 5\n",
    "# target_length = sample_rate * duration\n",
    "\n",
    "# waveform_angry, sample_rate_angry = librosa.load(angry_sample['path'], sr=sample_rate, mono=True)\n",
    "# waveform_angry = librosa.util.fix_length(waveform_angry, size=target_length)\n",
    "\n",
    "# waveform_sad, sample_rate_sad = librosa.load(sad_sample['path'], sr=sample_rate, mono=True)\n",
    "# waveform_sad = librosa.util.fix_length(waveform_sad, size=target_length)\n",
    "\n",
    "# zcr_angry, energy_angry, mfccs_angry = extract_time_features(waveform_angry, sample_rate)\n",
    "# zcr_sad, energy_sad, mfccs_sad = extract_time_features(waveform_sad, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0024cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"ANGRY:\")\n",
    "# ipd.display(ipd.Audio(angry_sample['path']))\n",
    "\n",
    "# # Play sad sample\n",
    "# print(\"SAD:\")\n",
    "# ipd.display(ipd.Audio(sad_sample['path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0195046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\nFeature Shapes:\")\n",
    "# print(f\"  ZCR:    {zcr_angry.shape}\")\n",
    "# print(f\"  Energy: {energy_angry.shape}\")\n",
    "# print(f\"  MFCCs:  {mfccs_angry.shape}\")\n",
    "# print(f\"  Combined: ({1 + 1 + 13}, {zcr_angry.shape[1]}) = (15, {zcr_angry.shape[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8718d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============== PLOT COMPARISON ==============\n",
    "# fig, axes = plt.subplots(4, 2, figsize=(16, 14))\n",
    "\n",
    "# # Column 0: ANGRY | Column 1: SAD\n",
    "\n",
    "# # Row 0: Waveform\n",
    "# axes[0, 0].plot(waveform_angry, color='red', alpha=0.7)\n",
    "# axes[0, 0].set_title('ANGRY - Waveform', fontsize=12, fontweight='bold')\n",
    "# axes[0, 0].set_xlabel('Samples')\n",
    "# axes[0, 0].set_ylabel('Amplitude')\n",
    "\n",
    "# axes[0, 1].plot(waveform_sad, color='blue', alpha=0.7)\n",
    "# axes[0, 1].set_title('SAD - Waveform', fontsize=12, fontweight='bold')\n",
    "# axes[0, 1].set_xlabel('Samples')\n",
    "# axes[0, 1].set_ylabel('Amplitude')\n",
    "\n",
    "# # Row 1: Zero Crossing Rate\n",
    "# axes[1, 0].plot(zcr_angry[0], color='red', alpha=0.7)\n",
    "# axes[1, 0].set_title(f'ANGRY - Zero Crossing Rate\\nMean: {zcr_angry.mean():.4f}', fontsize=11)\n",
    "# axes[1, 0].set_xlabel('Time Frames')\n",
    "# axes[1, 0].set_ylabel('ZCR')\n",
    "# axes[1, 0].set_ylim(0, 0.3)\n",
    "\n",
    "# axes[1, 1].plot(zcr_sad[0], color='blue', alpha=0.7)\n",
    "# axes[1, 1].set_title(f'SAD - Zero Crossing Rate\\nMean: {zcr_sad.mean():.4f}', fontsize=11)\n",
    "# axes[1, 1].set_xlabel('Time Frames')\n",
    "# axes[1, 1].set_ylabel('ZCR')\n",
    "# axes[1, 1].set_ylim(0, 0.3)\n",
    "\n",
    "# # Row 2: RMS Energy\n",
    "# axes[2, 0].plot(energy_angry[0], color='red', alpha=0.7)\n",
    "# axes[2, 0].fill_between(range(len(energy_angry[0])), energy_angry[0], alpha=0.3, color='red')\n",
    "# axes[2, 0].set_title(f'ANGRY - RMS Energy\\nMean: {energy_angry.mean():.4f}', fontsize=11)\n",
    "# axes[2, 0].set_xlabel('Time Frames')\n",
    "# axes[2, 0].set_ylabel('Energy')\n",
    "\n",
    "# axes[2, 1].plot(energy_sad[0], color='blue', alpha=0.7)\n",
    "# axes[2, 1].fill_between(range(len(energy_sad[0])), energy_sad[0], alpha=0.3, color='blue')\n",
    "# axes[2, 1].set_title(f'SAD - RMS Energy\\nMean: {energy_sad.mean():.4f}', fontsize=11)\n",
    "# axes[2, 1].set_xlabel('Time Frames')\n",
    "# axes[2, 1].set_ylabel('Energy')\n",
    "\n",
    "# # Row 3: MFCCs\n",
    "# img1 = librosa.display.specshow(mfccs_angry, x_axis='time', sr=sample_rate, hop_length=512, ax=axes[3, 0])\n",
    "# axes[3, 0].set_title('ANGRY - MFCCs (13 coefficients)', fontsize=11)\n",
    "# axes[3, 0].set_ylabel('MFCC Coefficient')\n",
    "# fig.colorbar(img1, ax=axes[3, 0], format='%+2.0f')\n",
    "\n",
    "# img2 = librosa.display.specshow(mfccs_sad, x_axis='time', sr=sample_rate, hop_length=512, ax=axes[3, 1])\n",
    "# axes[3, 1].set_title('SAD - MFCCs (13 coefficients)', fontsize=11)\n",
    "# axes[3, 1].set_ylabel('MFCC Coefficient')\n",
    "# fig.colorbar(img2, ax=axes[3, 1], format='%+2.0f')\n",
    "\n",
    "# plt.suptitle('Time-Domain Features: ANGRY vs SAD', fontsize=14, fontweight='bold')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('time_domain_comparison.png', dpi=150)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "198bed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, csv_path, augment=False):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Emotion to number mapping\n",
    "        self.emotion_to_idx = {\n",
    "            'angry': 0,\n",
    "            'disgust': 1,\n",
    "            'fearful': 2,\n",
    "            'happy': 3,\n",
    "            'neutral': 4,\n",
    "            'sad': 5\n",
    "        }\n",
    "        \n",
    "        # Audio settings (UPDATED to match reference)\n",
    "        self.sample_rate = 22050    # Changed from 16000\n",
    "        self.duration = 5           # Changed from 3\n",
    "        self.target_length = self.sample_rate * self.duration  # 110250\n",
    "        \n",
    "        # Mel-spectrogram settings\n",
    "        self.n_mels = 128\n",
    "        self.n_fft = 2048\n",
    "        self.hop_length = 512\n",
    "        self.n_mfcc = 13  # Number of MFCC coefficients\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # 1. Load audio\n",
    "        waveform, sr = librosa.load(row['path'], sr=self.sample_rate, mono=True)\n",
    "        \n",
    "        # 2. Fix length to exactly 5 seconds (110250 samples at 22.05kHz)\n",
    "        if len(waveform) < self.target_length:\n",
    "            padding = self.target_length - len(waveform)\n",
    "            offset = padding // 2\n",
    "            waveform = np.pad(waveform, (offset, padding - offset), 'constant')\n",
    "        else:\n",
    "            waveform = waveform[:self.target_length]        \n",
    "        \n",
    "        if self.augment:\n",
    "            # Time shift\n",
    "            shift = np.random.randint(-8000, 8000)\n",
    "            waveform = np.roll(waveform, shift)\n",
    "            \n",
    "            # Add noise\n",
    "            noise = np.random.normal(0, 0.005, waveform.shape)\n",
    "            waveform = waveform + noise\n",
    "            \n",
    "            # Random volume\n",
    "            volume = np.random.uniform(0.8, 1.2)\n",
    "            waveform = waveform * volume\n",
    "    \n",
    "        # 4. Extract time-domain features\n",
    "        # Zero Crossing Rate: (1, 216)\n",
    "        zcr = librosa.feature.zero_crossing_rate(\n",
    "            waveform, frame_length=self.n_fft, hop_length=self.hop_length\n",
    "        )\n",
    "        \n",
    "        # RMS Energy: (1, 216)\n",
    "        energy = librosa.feature.rms(\n",
    "            y=waveform, frame_length=self.n_fft, hop_length=self.hop_length\n",
    "        )\n",
    "        \n",
    "        # MFCCs: (13, 216)\n",
    "        mfccs = librosa.feature.mfcc(\n",
    "            y=waveform, sr=self.sample_rate, \n",
    "            n_fft=self.n_fft, hop_length=self.hop_length, n_mfcc=self.n_mfcc\n",
    "        )\n",
    "\n",
    "        # 5. Stack features: (15, 216)\n",
    "        features = np.vstack([zcr, energy, mfccs])\n",
    "\n",
    "        # 7. Convert to tensor\n",
    "        # Shape: (15, 216) - no channel dimension needed for 1D CNN\n",
    "        features_tensor = torch.FloatTensor(features)\n",
    "        \n",
    "        # 8. Get label\n",
    "        label = self.emotion_to_idx[row['emotion']]\n",
    "        \n",
    "        return features_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d7acdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef88dd02",
   "metadata": {},
   "source": [
    "# Testing starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "135ed635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36bbc558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "train_dataset = EmotionDataset('train_split.csv')\n",
    "validation_dataset = EmotionDataset('val_split.csv')  # No augmentation for validation\n",
    "test_dataset = EmotionDataset('test_split.csv')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5beaf9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "happy      136\n",
      "sad        136\n",
      "fearful    136\n",
      "angry      136\n",
      "disgust    136\n",
      "neutral    107\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "validation_Csv = pd.read_csv('val_split.csv')\n",
    "print(validation_Csv[\"emotion\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3722264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6305 samples\n",
      "Validation: 787 samples\n",
      "Test: 1406 samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Validation: {len(validation_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c39e120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "happy      136\n",
      "sad        136\n",
      "fearful    136\n",
      "angry      136\n",
      "disgust    136\n",
      "neutral    107\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('val_split.csv')\n",
    "print(df[\"emotion\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c4d4e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, validation_loader, epochs=50, lr=0.001, patience=10, device='cuda', weight_decay=1e-4):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_model_state = None\n",
    "    history = {'train_acc': [], 'val_acc': [], 'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in validation_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        val_loss = val_loss / len(validation_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}% | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopping(val_loss):\n",
    "            print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return best_acc, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25ed2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 256, 216]          11,776\n",
      "       BatchNorm1d-2             [-1, 256, 216]             512\n",
      "              ReLU-3             [-1, 256, 216]               0\n",
      "         AvgPool1d-4             [-1, 256, 108]               0\n",
      "            Conv1d-5             [-1, 256, 108]         196,864\n",
      "       BatchNorm1d-6             [-1, 256, 108]             512\n",
      "              ReLU-7             [-1, 256, 108]               0\n",
      "            Conv1d-8             [-1, 256, 108]         327,936\n",
      "       BatchNorm1d-9             [-1, 256, 108]             512\n",
      "             ReLU-10             [-1, 256, 108]               0\n",
      "           Conv1d-11             [-1, 128, 108]          98,432\n",
      "      BatchNorm1d-12             [-1, 128, 108]             256\n",
      "             ReLU-13             [-1, 128, 108]               0\n",
      "AdaptiveAvgPool1d-14               [-1, 128, 1]               0\n",
      "          Flatten-15                  [-1, 128]               0\n",
      "           Linear-16                   [-1, 64]           8,256\n",
      "             ReLU-17                   [-1, 64]               0\n",
      "          Dropout-18                   [-1, 64]               0\n",
      "           Linear-19                    [-1, 6]             390\n",
      "================================================================\n",
      "Total params: 645,446\n",
      "Trainable params: 645,446\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.06\n",
      "Params size (MB): 2.46\n",
      "Estimated Total Size (MB): 5.54\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\anaconda\\envs\\alex\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Acc: 37.32% | Val Acc: 37.74% | Train Loss: 1.5072 | Val Loss: 1.5232\n",
      "Epoch 2/50 | Train Acc: 43.58% | Val Acc: 38.12% | Train Loss: 1.4105 | Val Loss: 1.4333\n",
      "Epoch 3/50 | Train Acc: 46.47% | Val Acc: 42.95% | Train Loss: 1.3420 | Val Loss: 1.4486\n",
      "Epoch 4/50 | Train Acc: 48.75% | Val Acc: 44.35% | Train Loss: 1.2922 | Val Loss: 1.3822\n",
      "Epoch 5/50 | Train Acc: 49.90% | Val Acc: 40.91% | Train Loss: 1.2616 | Val Loss: 1.4880\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m stats \u001b[38;5;241m=\u001b[39m summary(model, input_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m216\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m best_acc, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[55], line 32\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, validation_loader, epochs, lr, patience, device, weight_decay)\u001b[0m\n\u001b[0;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 32\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     34\u001b[0m train_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TimeCNN(num_classes=6).to(device)\n",
    "            \n",
    "# model summary\n",
    "stats = summary(model, input_size=(15, 216))\n",
    "\n",
    "# Train\n",
    "best_acc, history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=validation_loader,\n",
    "    epochs=50,\n",
    "    lr=1e-4,\n",
    "    patience=10,\n",
    "    device=device,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "print(f\"Best Accuracy: {best_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
