{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8fcdf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "from architectures.TimeCNN import TimeCNN\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b88e88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAVDESS\n",
    "\n",
    "path = 'datasets/archive/audio_speech_actors_01-24'\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "val_data = []\n",
    "\n",
    "# Speaker split (gender-balanced)\n",
    "train_speakers = [\"01\",\"03\",\"05\",\"07\",\"09\",\"11\",\"13\",\"15\",\"17\",\"02\",\"04\",\"06\",\"08\",\"10\",\"12\",\"14\",\"16\"]\n",
    "val_speakers = [\"19\",\"20\",\"18\"]  # added validation split, took speakers form train split\n",
    "test_speakers = [\"21\",\"23\",\"22\",\"24\"]\n",
    "\n",
    "emotion_map = {\n",
    "    \"01\": \"neutral\",\n",
    "    # \"02\": \"calm\",  -> remove not in crema-d\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    # \"08\": \"surprised\" -> remove not in crema-d\n",
    "}\n",
    "\n",
    "for folder in sorted(os.listdir(path)):\n",
    "    folder_path = os.path.join(path, folder)\n",
    "    for file in os.listdir(folder_path):\n",
    "        match = re.match(r\"(\\d+)-(\\d+)-(\\d+)-(\\d+)-(\\d+)-(\\d+)-(\\d+)\\.wav\", file)\n",
    "        if match:\n",
    "            filepath = str(Path(path) / folder / file)\n",
    "            indicators = match.groups()\n",
    "            emotion_code = indicators[2]\n",
    "            actor_id = indicators[-1]\n",
    "            gender = 'Female' if int(indicators[-1]) % 2 == 0 else 'Male'\n",
    "            if emotion_code not in emotion_map:\n",
    "                continue\n",
    "\n",
    "            record = {\n",
    "                \"path\": filepath,\n",
    "                \"emotion\": emotion_map[emotion_code],\n",
    "                \"gender\": gender,\n",
    "                \"source\": \"RAVDESS\"\n",
    "            }\n",
    "\n",
    "            if actor_id in train_speakers:\n",
    "                train_data.append(record)\n",
    "            elif actor_id in val_speakers:\n",
    "                val_data.append(record)\n",
    "            else:\n",
    "                test_data.append(record)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "val_df = pd.DataFrame(val_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "train_df.to_csv(\"train_split.csv\", index=False)\n",
    "val_df.to_csv(\"val_split.csv\", index=False)\n",
    "test_df.to_csv(\"test_split.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1148a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(748, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9991b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ActorID  Age     Sex              Race     Ethnicity\n",
      "0     1001   51    Male         Caucasian  Not Hispanic\n",
      "1     1002   21  Female         Caucasian  Not Hispanic\n",
      "2     1003   21  Female         Caucasian  Not Hispanic\n",
      "3     1004   42  Female         Caucasian  Not Hispanic\n",
      "4     1005   29    Male  African American  Not Hispanic\n",
      "5     1006   58  Female         Caucasian  Not Hispanic\n",
      "6     1007   38  Female  African American  Not Hispanic\n",
      "7     1008   46  Female         Caucasian  Not Hispanic\n",
      "8     1009   24  Female         Caucasian  Not Hispanic\n",
      "9     1010   27  Female         Caucasian  Not Hispanic\n",
      "Index(['ActorID', 'Age', 'Sex', 'Race', 'Ethnicity'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "demo_df = pd.read_csv('datasets/CREMA-D/VideoDemographics.csv')\n",
    "print(demo_df.head(10))\n",
    "print(demo_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ea612c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total actors: 91\n",
      "Males: 48\n",
      "Females: 43\n"
     ]
    }
   ],
   "source": [
    "# CREMA-D\n",
    "\n",
    "path = 'datasets/CREMA-D/AudioWAV'\n",
    "\n",
    "# Load demographics for gender mapping\n",
    "demo_df = pd.read_csv('datasets/CREMA-D/VideoDemographics.csv')\n",
    "gender_map = dict(zip(demo_df['ActorID'], demo_df['Sex']))\n",
    "\n",
    "# Check how many actors\n",
    "print(f\"Total actors: {len(gender_map)}\")\n",
    "print(f\"Males: {list(gender_map.values()).count('Male')}\")\n",
    "print(f\"Females: {list(gender_map.values()).count('Female')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "711d485c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train speakers: 68 (34M, 34F)\n",
      "Validation speakers: 8 (4M, 4F)\n",
      "Test speakers: 15 (10M, 5F)\n",
      "\n",
      "CREMA-D TRAIN:\n",
      "  Samples: 5557\n",
      "  By gender: {'Male': 2782, 'Female': 2775}\n",
      "\n",
      "CREMA-D VALIDATION:\n",
      "  Samples: 655\n",
      " by gender: {'Male': 328, 'Female': 327}\n",
      "\n",
      "CREMA-D TEST:\n",
      "  Samples: 1230\n",
      "  By gender: {'Male': 820, 'Female': 410}\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "val_data = []\n",
    "\n",
    "emotion_map = {\n",
    "    \"ANG\": \"angry\",\n",
    "    \"DIS\": \"disgust\",\n",
    "    \"FEA\": \"fearful\",\n",
    "    \"HAP\": \"happy\",\n",
    "    \"NEU\": \"neutral\",\n",
    "    \"SAD\": \"sad\"\n",
    "}\n",
    "\n",
    "# Get all actor IDs and split by gender\n",
    "all_actors = demo_df['ActorID'].tolist()\n",
    "male_actors = demo_df[demo_df['Sex'] == 'Male']['ActorID'].tolist()\n",
    "female_actors = demo_df[demo_df['Sex'] == 'Female']['ActorID'].tolist()\n",
    "\n",
    "# 80/20 split per gender\n",
    "train_males = male_actors[:34]      # 34 males for train\n",
    "val_males = male_actors[34:38]      # 4 males for validation\n",
    "test_males = male_actors[38:]       # 10 males for test\n",
    "\n",
    "train_females = female_actors[:34]  # 34 females for train  \n",
    "val_females = female_actors[34:38]  # 4 females for validation\n",
    "test_females = female_actors[38:]   # 5 females for test\n",
    "\n",
    "train_speakers = train_males + train_females  # 72 speakers\n",
    "val_speakers = val_males + val_females        # 8 speakers\n",
    "test_speakers = test_males + test_females      # 19 speakers\n",
    "\n",
    "print(f\"\\nTrain speakers: {len(train_speakers)} ({len(train_males)}M, {len(train_females)}F)\")\n",
    "print(f\"Validation speakers: {len(val_speakers)} ({len(val_males)}M, {len(val_females)}F)\")\n",
    "print(f\"Test speakers: {len(test_speakers)} ({len(test_males)}M, {len(test_females)}F)\")\n",
    "\n",
    "# Parse audio files\n",
    "for file in os.listdir(path):\n",
    "    if not file.endswith('.wav'):\n",
    "        continue\n",
    "        \n",
    "    # Filename: 1001_IEO_ANG_HI.wav\n",
    "    parts = file.replace('.wav', '').split('_')\n",
    "    \n",
    "    if len(parts) < 4:\n",
    "        continue\n",
    "    \n",
    "    actor_id = int(parts[0])\n",
    "    emotion_code = parts[2]\n",
    "    \n",
    "    if emotion_code not in emotion_map:\n",
    "        continue\n",
    "    \n",
    "    filepath = str(Path(path) / file)\n",
    "    gender = gender_map.get(actor_id, 'Unknown')\n",
    "    \n",
    "    record = {\n",
    "        \"path\": filepath,\n",
    "        \"emotion\": emotion_map[emotion_code],\n",
    "        \"gender\": gender,\n",
    "        \"source\": \"CREMA-D\"\n",
    "    }\n",
    "    \n",
    "    if actor_id in train_speakers:\n",
    "        train_data.append(record)\n",
    "    elif actor_id in val_speakers:\n",
    "        val_data.append(record)\n",
    "    elif actor_id in test_speakers:\n",
    "        test_data.append(record)\n",
    "\n",
    "# Create DataFrames\n",
    "train_df_cremad = pd.DataFrame(train_data)\n",
    "val_df_cremad = pd.DataFrame(val_data)\n",
    "test_df_cremad = pd.DataFrame(test_data)\n",
    "\n",
    "# Verify\n",
    "print(\"\\nCREMA-D TRAIN:\")\n",
    "print(f\"  Samples: {len(train_df_cremad)}\")\n",
    "print(f\"  By gender: {train_df_cremad['gender'].value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\nCREMA-D VALIDATION:\")\n",
    "print(f\"  Samples: {len(val_df_cremad)}\")\n",
    "print(f\" by gender: {val_df_cremad['gender'].value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\nCREMA-D TEST:\")\n",
    "print(f\"  Samples: {len(test_df_cremad)}\")\n",
    "print(f\"  By gender: {test_df_cremad['gender'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b6266a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL TRAIN: 6305\n",
      "FINAL VALIDATION: 787\n",
      "FINAL TEST: 1406\n"
     ]
    }
   ],
   "source": [
    "train_df_combined = pd.concat([train_df, train_df_cremad], ignore_index=True)\n",
    "val_df_combined = pd.concat([val_df, val_df_cremad], ignore_index=True)\n",
    "test_df_combined = pd.concat([test_df, test_df_cremad], ignore_index=True)\n",
    "\n",
    "train_df_combined.to_csv(\"train_split.csv\", index=False)\n",
    "val_df_combined.to_csv(\"val_split.csv\", index=False)\n",
    "test_df_combined.to_csv(\"test_split.csv\", index=False)\n",
    "\n",
    "print(f\"\\nFINAL TRAIN: {len(train_df_combined)}\")\n",
    "print(f\"FINAL VALIDATION: {len(val_df_combined)}\")\n",
    "print(f\"FINAL TEST: {len(test_df_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "469d1a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'happy' 'sad' 'angry' 'fearful' 'disgust']\n",
      "['neutral' 'happy' 'sad' 'angry' 'fearful' 'disgust']\n",
      "['neutral' 'happy' 'sad' 'angry' 'fearful' 'disgust']\n"
     ]
    }
   ],
   "source": [
    "# quick check to ensure \"calm\" emotion is removed\n",
    "print(train_df['emotion'].unique())\n",
    "print(val_df['emotion'].unique())\n",
    "print(test_df['emotion'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a3f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_path = train_df.iloc[0]['path']\n",
    "# sample_emotion = train_df.iloc[0]['emotion']\n",
    "# print (f\"Sample path: {sample_path},\\nemotion: {sample_emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32f92c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sample_rate_1 = 16000\n",
    "# sample_rate_2 = 22050\n",
    "# duration_1 = 3\n",
    "# duration_2 = 5\n",
    "\n",
    "\n",
    "# waveform_1, sr_1 = librosa.load(sample_path, sr=sample_rate_1)\n",
    "# print(f\"\\nOriginal_1:\")\n",
    "# print(f\"  Sample rate: {sr_1} Hz\")\n",
    "# print(f\"  Shape: {waveform_1.shape}\")\n",
    "# print(f\"  Duration: {len(waveform_1)/sr_1:.2f} seconds\")\n",
    "\n",
    "# waveform_2, sr_2 = librosa.load(sample_path, sr=sample_rate_2)\n",
    "# print(f\"\\nOriginal_2:\")\n",
    "# print(f\"  Sample rate: {sr_2} Hz\")\n",
    "# print(f\"  Shape: {waveform_2.shape}\")\n",
    "# print(f\"  Duration: {len(waveform_2)/sr_2:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48c9eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_length_1 = sample_rate_1 * duration_1  # 3 seconds at 16kHz\n",
    "# target_length_2 = sample_rate_2 * duration_2  # 5 seconds at 22.05kHz\n",
    "\n",
    "# waveform_1 = librosa.util.fix_length(waveform_1, size=target_length_1)\n",
    "# waveform_2 = librosa.util.fix_length(waveform_2, size=target_length_2)\n",
    "\n",
    "# print(f\"Final waveform_1 shape: {waveform_1.shape}\")\n",
    "# print(f\"Final waveform_2 shape: {waveform_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7709bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mel_spec_1 = librosa.feature.melspectrogram(\n",
    "#     y=waveform_1,\n",
    "#     sr=sample_rate_1,\n",
    "#     n_mels = 128,\n",
    "#     n_fft=2048,\n",
    "#     hop_length=512\n",
    "# )\n",
    "# print(f\"\\nMel-spectrogram shape: {mel_spec_1.shape}\")\n",
    "\n",
    "# mel_spec_2 = librosa.feature.melspectrogram(\n",
    "#     y=waveform_2,\n",
    "#     sr=sample_rate_2,\n",
    "#     n_mels = 128,\n",
    "#     n_fft=2048,\n",
    "#     hop_length=512\n",
    "# )\n",
    "# print(f\"\\nMel-spectrogram shape: {mel_spec_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "090be072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mel_spec_db_1 = librosa.power_to_db(mel_spec_1, ref=np.max)\n",
    "# print(f\"Mel-spectrogram (dB) shape: {mel_spec_db_1.shape}\")\n",
    "# mel_spec_db_2 = librosa.power_to_db(mel_spec_2, ref=np.max)\n",
    "# print(f\"Mel-spectrogram (dB) shape: {mel_spec_db_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e753fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============== PLOT COMPARISON ==============\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# # Row 1: Waveforms\n",
    "# axes[0, 0].plot(waveform_1)\n",
    "# axes[0, 0].set_title(f'Waveform - 16kHz, 3 sec\\nSamples: {len(waveform_1):,}')\n",
    "# axes[0, 0].set_xlabel('Samples')\n",
    "# axes[0, 0].set_ylabel('Amplitude')\n",
    "\n",
    "# axes[0, 1].plot(waveform_2)\n",
    "# axes[0, 1].set_title(f'Waveform - 22kHz, 5 sec\\nSamples: {len(waveform_2):,}')\n",
    "# axes[0, 1].set_xlabel('Samples')\n",
    "# axes[0, 1].set_ylabel('Amplitude')\n",
    "\n",
    "# # Row 2: Mel-Spectrograms (dB)\n",
    "# img1 = librosa.display.specshow(\n",
    "#     mel_spec_db_1, x_axis='time', y_axis='mel', sr=sample_rate_1, ax=axes[1, 0]\n",
    "# )\n",
    "# axes[1, 0].set_title(f'Mel-Spectrogram (YOUR SETTINGS)\\nShape: {mel_spec_db_1.shape}')\n",
    "# fig.colorbar(img1, ax=axes[1, 0], format='%+2.0f dB')\n",
    "\n",
    "# img2 = librosa.display.specshow(\n",
    "#     mel_spec_db_2, x_axis='time', y_axis='mel', sr=sample_rate_2, ax=axes[1, 1]\n",
    "# )\n",
    "# axes[1, 1].set_title(f'Mel-Spectrogram (REFERENCE SETTINGS)\\nShape: {mel_spec_db_2.shape}')\n",
    "# fig.colorbar(img2, ax=axes[1, 1], format='%+2.0f dB')\n",
    "\n",
    "# plt.suptitle(f'Comparison: {sample_emotion.upper()}', fontsize=14, fontweight='bold')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('audio_comparison.png', dpi=150)\n",
    "# plt.show()\n",
    "\n",
    "# # ============== SUMMARY ==============\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# print(\"COMPARISON SUMMARY\")\n",
    "# print(\"=\" * 60)\n",
    "# print(f\"\\n{'Setting':<25} {'Yours':<20} {'Reference':<20}\")\n",
    "# print(\"-\" * 60)\n",
    "# print(f\"{'Sample Rate':<25} {sample_rate_1:,} Hz{'':<10} {sample_rate_2:,} Hz\")\n",
    "# print(f\"{'Duration':<25} {duration_1} sec{'':<15} {duration_2} sec\")\n",
    "# print(f\"{'Total Samples':<25} {len(waveform_1):,}{'':<13} {len(waveform_2):,}\")\n",
    "# print(f\"{'Spectrogram Shape':<25} {mel_spec_db_1.shape}{'':<11} {mel_spec_db_2.shape}\")\n",
    "# print(f\"{'Time Frames':<25} {mel_spec_db_1.shape[1]}{'':<17} {mel_spec_db_2.shape[1]}\")\n",
    "# print(\"-\" * 60)\n",
    "# print(f\"\\nReference has {mel_spec_db_2.shape[1] / mel_spec_db_1.shape[1]:.1f}x more temporal information!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16faa9",
   "metadata": {},
   "source": [
    "128 mel bands - frequency range leading up to 48000hz (3 sec time)\n",
    "\n",
    "94 time windows\n",
    "\n",
    "hence the shape (128, 94)\n",
    "\n",
    "notice the power db graph (scale adjusted for human audible samples) - we set the loudest volume (highest amplitude) to 0, closest to that will be loude voice represented with bright colors and then leading away from it is quiter around -80 db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80800e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_time_features(waveform, sr):\n",
    "#     # Zero Crossing Rate\n",
    "#     zcr = librosa.feature.zero_crossing_rate(\n",
    "#         waveform, frame_length=2048, hop_length=512\n",
    "#     )\n",
    "    \n",
    "#     # RMS Energy\n",
    "#     energy = librosa.feature.rms(\n",
    "#         y=waveform, frame_length=2048, hop_length=512\n",
    "#     )\n",
    "    \n",
    "#     # MFCCs\n",
    "#     mfccs = librosa.feature.mfcc(\n",
    "#         y=waveform, sr=sr, n_fft=2048, hop_length=512, n_mfcc=13\n",
    "#     )\n",
    "    \n",
    "#     return zcr, energy, mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb67c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# angry_sample  = train_df[train_df['emotion'] == 'angry'].iloc[0]\n",
    "# sad_sample = train_df[train_df['emotion'] == 'sad'].iloc[0]\n",
    "\n",
    "# sample_rate = 22050\n",
    "# duration = 5\n",
    "# target_length = sample_rate * duration\n",
    "\n",
    "# waveform_angry, sample_rate_angry = librosa.load(angry_sample['path'], sr=sample_rate, mono=True)\n",
    "# waveform_angry = librosa.util.fix_length(waveform_angry, size=target_length)\n",
    "\n",
    "# waveform_sad, sample_rate_sad = librosa.load(sad_sample['path'], sr=sample_rate, mono=True)\n",
    "# waveform_sad = librosa.util.fix_length(waveform_sad, size=target_length)\n",
    "\n",
    "# zcr_angry, energy_angry, mfccs_angry = extract_time_features(waveform_angry, sample_rate)\n",
    "# zcr_sad, energy_sad, mfccs_sad = extract_time_features(waveform_sad, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0024cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"ANGRY:\")\n",
    "# ipd.display(ipd.Audio(angry_sample['path']))\n",
    "\n",
    "# # Play sad sample\n",
    "# print(\"SAD:\")\n",
    "# ipd.display(ipd.Audio(sad_sample['path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0195046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\nFeature Shapes:\")\n",
    "# print(f\"  ZCR:    {zcr_angry.shape}\")\n",
    "# print(f\"  Energy: {energy_angry.shape}\")\n",
    "# print(f\"  MFCCs:  {mfccs_angry.shape}\")\n",
    "# print(f\"  Combined: ({1 + 1 + 13}, {zcr_angry.shape[1]}) = (15, {zcr_angry.shape[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8718d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============== PLOT COMPARISON ==============\n",
    "# fig, axes = plt.subplots(4, 2, figsize=(16, 14))\n",
    "\n",
    "# # Column 0: ANGRY | Column 1: SAD\n",
    "\n",
    "# # Row 0: Waveform\n",
    "# axes[0, 0].plot(waveform_angry, color='red', alpha=0.7)\n",
    "# axes[0, 0].set_title('ANGRY - Waveform', fontsize=12, fontweight='bold')\n",
    "# axes[0, 0].set_xlabel('Samples')\n",
    "# axes[0, 0].set_ylabel('Amplitude')\n",
    "\n",
    "# axes[0, 1].plot(waveform_sad, color='blue', alpha=0.7)\n",
    "# axes[0, 1].set_title('SAD - Waveform', fontsize=12, fontweight='bold')\n",
    "# axes[0, 1].set_xlabel('Samples')\n",
    "# axes[0, 1].set_ylabel('Amplitude')\n",
    "\n",
    "# # Row 1: Zero Crossing Rate\n",
    "# axes[1, 0].plot(zcr_angry[0], color='red', alpha=0.7)\n",
    "# axes[1, 0].set_title(f'ANGRY - Zero Crossing Rate\\nMean: {zcr_angry.mean():.4f}', fontsize=11)\n",
    "# axes[1, 0].set_xlabel('Time Frames')\n",
    "# axes[1, 0].set_ylabel('ZCR')\n",
    "# axes[1, 0].set_ylim(0, 0.3)\n",
    "\n",
    "# axes[1, 1].plot(zcr_sad[0], color='blue', alpha=0.7)\n",
    "# axes[1, 1].set_title(f'SAD - Zero Crossing Rate\\nMean: {zcr_sad.mean():.4f}', fontsize=11)\n",
    "# axes[1, 1].set_xlabel('Time Frames')\n",
    "# axes[1, 1].set_ylabel('ZCR')\n",
    "# axes[1, 1].set_ylim(0, 0.3)\n",
    "\n",
    "# # Row 2: RMS Energy\n",
    "# axes[2, 0].plot(energy_angry[0], color='red', alpha=0.7)\n",
    "# axes[2, 0].fill_between(range(len(energy_angry[0])), energy_angry[0], alpha=0.3, color='red')\n",
    "# axes[2, 0].set_title(f'ANGRY - RMS Energy\\nMean: {energy_angry.mean():.4f}', fontsize=11)\n",
    "# axes[2, 0].set_xlabel('Time Frames')\n",
    "# axes[2, 0].set_ylabel('Energy')\n",
    "\n",
    "# axes[2, 1].plot(energy_sad[0], color='blue', alpha=0.7)\n",
    "# axes[2, 1].fill_between(range(len(energy_sad[0])), energy_sad[0], alpha=0.3, color='blue')\n",
    "# axes[2, 1].set_title(f'SAD - RMS Energy\\nMean: {energy_sad.mean():.4f}', fontsize=11)\n",
    "# axes[2, 1].set_xlabel('Time Frames')\n",
    "# axes[2, 1].set_ylabel('Energy')\n",
    "\n",
    "# # Row 3: MFCCs\n",
    "# img1 = librosa.display.specshow(mfccs_angry, x_axis='time', sr=sample_rate, hop_length=512, ax=axes[3, 0])\n",
    "# axes[3, 0].set_title('ANGRY - MFCCs (13 coefficients)', fontsize=11)\n",
    "# axes[3, 0].set_ylabel('MFCC Coefficient')\n",
    "# fig.colorbar(img1, ax=axes[3, 0], format='%+2.0f')\n",
    "\n",
    "# img2 = librosa.display.specshow(mfccs_sad, x_axis='time', sr=sample_rate, hop_length=512, ax=axes[3, 1])\n",
    "# axes[3, 1].set_title('SAD - MFCCs (13 coefficients)', fontsize=11)\n",
    "# axes[3, 1].set_ylabel('MFCC Coefficient')\n",
    "# fig.colorbar(img2, ax=axes[3, 1], format='%+2.0f')\n",
    "\n",
    "# plt.suptitle('Time-Domain Features: ANGRY vs SAD', fontsize=14, fontweight='bold')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('time_domain_comparison.png', dpi=150)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "198bed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, csv_path, augment=False, stats=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.augment = augment\n",
    "        self.stats = stats  # (mean, std) tuple, shape (15, 1) each\n",
    "        \n",
    "        # Emotion to number mapping\n",
    "        self.emotion_to_idx = {\n",
    "            'angry': 0,\n",
    "            'disgust': 1,\n",
    "            'fearful': 2,\n",
    "            'happy': 3,\n",
    "            'neutral': 4,\n",
    "            'sad': 5\n",
    "        }\n",
    "        \n",
    "        # Audio settings\n",
    "        self.sample_rate = 22050\n",
    "        self.duration = 5\n",
    "        self.target_length = self.sample_rate * self.duration  # 110250\n",
    "        \n",
    "        # Feature extraction settings\n",
    "        self.n_fft = 2048\n",
    "        self.hop_length = 512\n",
    "        self.n_mfcc = 13\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _extract_features(self, waveform):\n",
    "        zcr = librosa.feature.zero_crossing_rate(\n",
    "            waveform, frame_length=self.n_fft, hop_length=self.hop_length\n",
    "        )\n",
    "        energy = librosa.feature.rms(\n",
    "            y=waveform, frame_length=self.n_fft, hop_length=self.hop_length\n",
    "        )\n",
    "        mfccs = librosa.feature.mfcc(\n",
    "            y=waveform, sr=self.sample_rate,\n",
    "            n_fft=self.n_fft, hop_length=self.hop_length, n_mfcc=self.n_mfcc\n",
    "        )\n",
    "        \n",
    "        # Delta and delta-delta capture how features change over time\n",
    "        # Critical for emotion — it's not just pitch, it's how pitch moves\n",
    "        mfcc_delta = librosa.feature.delta(mfccs)\n",
    "        mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "        \n",
    "        # Also add spectral features\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(\n",
    "            y=waveform, sr=self.sample_rate, n_fft=self.n_fft, hop_length=self.hop_length\n",
    "        )\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(\n",
    "            y=waveform, sr=self.sample_rate, n_fft=self.n_fft, hop_length=self.hop_length\n",
    "        )  # (7, 216)\n",
    "        \n",
    "        # Stack: 1 + 1 + 13 + 13 + 13 + 1 + 7 = 49 channels\n",
    "        return np.vstack([zcr, energy, mfccs, mfcc_delta, mfcc_delta2, \n",
    "                        spectral_centroid, spectral_contrast])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # 1. Load audio\n",
    "        waveform, sr = librosa.load(row['path'], sr=self.sample_rate, mono=True)\n",
    "        \n",
    "        # 2. Fix length to exactly 5 seconds\n",
    "        if len(waveform) < self.target_length:\n",
    "            padding = self.target_length - len(waveform)\n",
    "            offset = padding // 2\n",
    "            waveform = np.pad(waveform, (offset, padding - offset), 'constant')\n",
    "        else:\n",
    "            waveform = waveform[:self.target_length]\n",
    "        \n",
    "        # 3. Augmentation (before feature extraction)\n",
    "        if self.augment:\n",
    "            if np.random.random() < 0.5:\n",
    "                shift = np.random.randint(-2000, 2000)\n",
    "                waveform = np.roll(waveform, shift)\n",
    "            \n",
    "            if np.random.random() < 0.5:\n",
    "                noise = np.random.normal(0, 0.003, waveform.shape)\n",
    "                waveform = waveform + noise\n",
    "            \n",
    "            if np.random.random() < 0.5:\n",
    "                volume = np.random.uniform(0.9, 1.1)\n",
    "                waveform = waveform * volume\n",
    "        \n",
    "        # 4. Extract features: (15, 216)\n",
    "        features = self._extract_features(waveform)\n",
    "        \n",
    "        # 5. Normalize\n",
    "        if self.stats is not None:\n",
    "            mean, std = self.stats\n",
    "            features = (features - mean) / (std + 1e-8)\n",
    "        \n",
    "        # 6. Convert to tensor\n",
    "        features_tensor = torch.FloatTensor(features)\n",
    "        \n",
    "        # 7. Get label\n",
    "        label = self.emotion_to_idx[row['emotion']]\n",
    "        \n",
    "        return features_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d7acdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef88dd02",
   "metadata": {},
   "source": [
    "# Testing starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "135ed635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7df0ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing training set statistics...\n",
      "Computed stats over 6305 samples\n",
      "Mean range: [-543.7072, 902.1108]\n",
      "Std range:  [0.0382, 1114.0896]\n"
     ]
    }
   ],
   "source": [
    "def compute_dataset_stats(csv_path, sample_rate=22050, duration=5, n_fft=2048, hop_length=512, n_mfcc=13):\n",
    "    \"\"\"Compute per-channel mean and std across the entire training set.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    target_length = sample_rate * duration\n",
    "    \n",
    "    all_features = []\n",
    "    \n",
    "    for idx in range(len(df)):\n",
    "        row = df.iloc[idx]\n",
    "        waveform, sr = librosa.load(row['path'], sr=sample_rate, mono=True)\n",
    "        \n",
    "        if len(waveform) < target_length:\n",
    "            padding = target_length - len(waveform)\n",
    "            offset = padding // 2\n",
    "            waveform = np.pad(waveform, (offset, padding - offset), 'constant')\n",
    "        else:\n",
    "            waveform = waveform[:target_length]\n",
    "        \n",
    "        zcr = librosa.feature.zero_crossing_rate(waveform, frame_length=n_fft, hop_length=hop_length)\n",
    "        energy = librosa.feature.rms(y=waveform, frame_length=n_fft, hop_length=hop_length)\n",
    "        mfccs = librosa.feature.mfcc(y=waveform, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc)\n",
    "        mfcc_delta = librosa.feature.delta(mfccs)\n",
    "        mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=waveform, sr=sample_rate, n_fft=n_fft, hop_length=hop_length)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=waveform, sr=sample_rate, n_fft=n_fft, hop_length=hop_length)\n",
    "        \n",
    "        features = np.vstack([zcr, energy, mfccs, mfcc_delta, mfcc_delta2, \n",
    "                              spectral_centroid, spectral_contrast])  # (49, 216)\n",
    "        all_features.append(features)\n",
    "    \n",
    "    all_features = np.stack(all_features)  # (N, 49, 216)\n",
    "    \n",
    "    mean = all_features.mean(axis=(0, 2), keepdims=True).squeeze(0)  # (49, 1)\n",
    "    std = all_features.std(axis=(0, 2), keepdims=True).squeeze(0)    # (49, 1)\n",
    "    \n",
    "    print(f\"Computed stats over {len(df)} samples\")\n",
    "    print(f\"Mean range: [{mean.min():.4f}, {mean.max():.4f}]\")\n",
    "    print(f\"Std range:  [{std.min():.4f}, {std.max():.4f}]\")\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "# Compute stats from training set ONLY\n",
    "print(\"Computing training set statistics...\")\n",
    "train_stats = compute_dataset_stats('train_split.csv')\n",
    "\n",
    "# Create datasets — pass same stats to all three\n",
    "train_dataset = EmotionDataset('train_split.csv', augment=True, stats=train_stats)\n",
    "validation_dataset = EmotionDataset('val_split.csv', augment=False, stats=train_stats)\n",
    "test_dataset = EmotionDataset('test_split.csv', augment=False, stats=train_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36bbc558",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5beaf9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "happy      136\n",
      "sad        136\n",
      "fearful    136\n",
      "angry      136\n",
      "disgust    136\n",
      "neutral    107\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "validation_Csv = pd.read_csv('val_split.csv')\n",
    "print(validation_Csv[\"emotion\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3722264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6305 samples\n",
      "Validation: 787 samples\n",
      "Test: 1406 samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Validation: {len(validation_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c39e120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "happy      136\n",
      "sad        136\n",
      "fearful    136\n",
      "angry      136\n",
      "disgust    136\n",
      "neutral    107\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('val_split.csv')\n",
    "print(df[\"emotion\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4d4e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, validation_loader, epochs=50, lr=0.001, patience=10, device='cuda', weight_decay=1e-4):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_model_state = None\n",
    "    history = {'train_acc': [], 'val_acc': [], 'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in validation_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        val_loss = val_loss / len(validation_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}% | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopping(val_loss):\n",
    "            print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return best_acc, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a25ed2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 64, 216]           9,472\n",
      "       BatchNorm1d-2              [-1, 64, 216]             128\n",
      "              ReLU-3              [-1, 64, 216]               0\n",
      "         AvgPool1d-4              [-1, 64, 108]               0\n",
      "            Conv1d-5             [-1, 128, 108]          24,704\n",
      "       BatchNorm1d-6             [-1, 128, 108]             256\n",
      "              ReLU-7             [-1, 128, 108]               0\n",
      "            Conv1d-8             [-1, 128, 108]          82,048\n",
      "       BatchNorm1d-9             [-1, 128, 108]             256\n",
      "             ReLU-10             [-1, 128, 108]               0\n",
      "           Conv1d-11             [-1, 128, 108]          49,280\n",
      "      BatchNorm1d-12             [-1, 128, 108]             256\n",
      "             ReLU-13             [-1, 128, 108]               0\n",
      "AdaptiveAvgPool1d-14               [-1, 128, 1]               0\n",
      "          Flatten-15                  [-1, 128]               0\n",
      "           Linear-16                   [-1, 64]           8,256\n",
      "             ReLU-17                   [-1, 64]               0\n",
      "          Dropout-18                   [-1, 64]               0\n",
      "           Linear-19                    [-1, 6]             390\n",
      "================================================================\n",
      "Total params: 175,046\n",
      "Trainable params: 175,046\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 1.32\n",
      "Params size (MB): 0.67\n",
      "Estimated Total Size (MB): 2.03\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\anaconda\\envs\\alex\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Acc: 26.49% | Val Acc: 32.78% | Train Loss: 1.7019 | Val Loss: 1.5788\n",
      "Epoch 2/50 | Train Acc: 36.97% | Val Acc: 37.99% | Train Loss: 1.5576 | Val Loss: 1.5075\n",
      "Epoch 3/50 | Train Acc: 41.21% | Val Acc: 41.04% | Train Loss: 1.4811 | Val Loss: 1.4553\n",
      "Epoch 4/50 | Train Acc: 42.81% | Val Acc: 41.42% | Train Loss: 1.4307 | Val Loss: 1.4109\n",
      "Epoch 5/50 | Train Acc: 46.38% | Val Acc: 42.06% | Train Loss: 1.3827 | Val Loss: 1.4037\n",
      "Epoch 6/50 | Train Acc: 48.15% | Val Acc: 46.12% | Train Loss: 1.3433 | Val Loss: 1.3339\n",
      "Epoch 7/50 | Train Acc: 49.64% | Val Acc: 49.68% | Train Loss: 1.3000 | Val Loss: 1.3223\n",
      "Epoch 8/50 | Train Acc: 50.79% | Val Acc: 48.79% | Train Loss: 1.2666 | Val Loss: 1.2924\n",
      "Epoch 9/50 | Train Acc: 53.93% | Val Acc: 48.67% | Train Loss: 1.2219 | Val Loss: 1.2769\n",
      "Epoch 10/50 | Train Acc: 55.31% | Val Acc: 52.48% | Train Loss: 1.1955 | Val Loss: 1.2398\n",
      "Epoch 11/50 | Train Acc: 55.19% | Val Acc: 48.41% | Train Loss: 1.1794 | Val Loss: 1.2884\n",
      "Epoch 12/50 | Train Acc: 57.72% | Val Acc: 52.22% | Train Loss: 1.1494 | Val Loss: 1.2124\n",
      "Epoch 13/50 | Train Acc: 57.92% | Val Acc: 49.56% | Train Loss: 1.1145 | Val Loss: 1.2803\n",
      "Epoch 14/50 | Train Acc: 59.51% | Val Acc: 51.72% | Train Loss: 1.0938 | Val Loss: 1.2203\n",
      "Epoch 15/50 | Train Acc: 59.83% | Val Acc: 52.48% | Train Loss: 1.0691 | Val Loss: 1.2095\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m stats \u001b[38;5;241m=\u001b[39m summary(model, input_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m49\u001b[39m, \u001b[38;5;241m216\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m best_acc, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[30], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, validation_loader, epochs, lr, patience, device, weight_decay)\u001b[0m\n\u001b[0;32m     19\u001b[0m train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     20\u001b[0m train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     23\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     24\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mm:\\anaconda\\envs\\alex\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mm:\\anaconda\\envs\\alex\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mm:\\anaconda\\envs\\alex\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mm:\\anaconda\\envs\\alex\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[20], line 88\u001b[0m, in \u001b[0;36mEmotionDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     85\u001b[0m         waveform \u001b[38;5;241m=\u001b[39m waveform \u001b[38;5;241m*\u001b[39m volume\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# 4. Extract features: (15, 216)\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# 5. Normalize\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[20], line 48\u001b[0m, in \u001b[0;36mEmotionDataset._extract_features\u001b[1;34m(self, waveform)\u001b[0m\n\u001b[0;32m     45\u001b[0m mfcc_delta2 \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mdelta(mfccs, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Also add spectral features\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m spectral_centroid \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectral_centroid\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhop_length\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m spectral_contrast \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mspectral_contrast(\n\u001b[0;32m     52\u001b[0m     y\u001b[38;5;241m=\u001b[39mwaveform, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_rate, n_fft\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_fft, hop_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhop_length\n\u001b[0;32m     53\u001b[0m )  \u001b[38;5;66;03m# (7, 216)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Stack: 1 + 1 + 13 + 13 + 13 + 1 + 7 = 49 channels\u001b[39;00m\n",
      "File \u001b[1;32mm:\\anaconda\\envs\\alex\\lib\\site-packages\\librosa\\feature\\spectral.py:157\u001b[0m, in \u001b[0;36mspectral_centroid\u001b[1;34m(y, sr, S, n_fft, hop_length, freq, win_length, window, center, pad_mode)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the spectral centroid.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mEach frame of a magnitude spectrogram is normalized and treated as a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m>>> ax.set(title='log Power spectrogram')\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# input is time domain:y or spectrogram:s\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m S, n_fft \u001b[38;5;241m=\u001b[39m \u001b[43m_spectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misrealobj(S):\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpectral centroid is only defined \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith real-valued input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n",
      "File \u001b[1;32mm:\\anaconda\\envs\\alex\\lib\\site-packages\\librosa\\core\\spectrum.py:2945\u001b[0m, in \u001b[0;36m_spectrogram\u001b[1;34m(y, S, n_fft, hop_length, power, win_length, window, center, pad_mode)\u001b[0m\n\u001b[0;32m   2939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2940\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\n\u001b[0;32m   2941\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput signal must be provided to compute a spectrogram\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2942\u001b[0m         )\n\u001b[0;32m   2943\u001b[0m     S \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2944\u001b[0m         np\u001b[38;5;241m.\u001b[39mabs(\n\u001b[1;32m-> 2945\u001b[0m             \u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2946\u001b[0m \u001b[43m                \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2947\u001b[0m \u001b[43m                \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2948\u001b[0m \u001b[43m                \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2949\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2950\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2951\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2952\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2953\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2954\u001b[0m         )\n\u001b[0;32m   2955\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m power\n\u001b[0;32m   2956\u001b[0m     )\n\u001b[0;32m   2958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m S, n_fft\n",
      "File \u001b[1;32mm:\\anaconda\\envs\\alex\\lib\\site-packages\\librosa\\core\\spectrum.py:387\u001b[0m, in \u001b[0;36mstft\u001b[1;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bl_s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], n_columns):\n\u001b[0;32m    385\u001b[0m     bl_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(bl_s \u001b[38;5;241m+\u001b[39m n_columns, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 387\u001b[0m     stft_matrix[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, bl_s \u001b[38;5;241m+\u001b[39m off_start : bl_t \u001b[38;5;241m+\u001b[39m off_start] \u001b[38;5;241m=\u001b[39m \u001b[43mfft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrfft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfft_window\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_frames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbl_s\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbl_t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stft_matrix\n",
      "File \u001b[1;32mm:\\anaconda\\envs\\alex\\lib\\site-packages\\scipy\\fft\\_backend.py:28\u001b[0m, in \u001b[0;36m_ScipyBackend.__ua_function__\u001b[1;34m(method, args, kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mm:\\anaconda\\envs\\alex\\lib\\site-packages\\scipy\\fft\\_basic_backend.py:91\u001b[0m, in \u001b[0;36mrfft\u001b[1;34m(x, n, axis, norm, overwrite_x, workers, plan)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrfft\u001b[39m(x, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     90\u001b[0m          overwrite_x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, plan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_execute_1D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrfft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pocketfft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrfft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m                       \u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mm:\\anaconda\\envs\\alex\\lib\\site-packages\\scipy\\fft\\_basic_backend.py:32\u001b[0m, in \u001b[0;36m_execute_1D\u001b[1;34m(func_str, pocketfft_func, x, n, axis, norm, overwrite_x, workers, plan)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_numpy(xp):\n\u001b[0;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpocketfft_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                          \u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m norm \u001b[38;5;241m=\u001b[39m _validate_fft_args(workers, plan, norm)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(xp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfft\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mm:\\anaconda\\envs\\alex\\lib\\site-packages\\scipy\\fft\\_pocketfft\\basic.py:61\u001b[0m, in \u001b[0;36mr2c\u001b[1;34m(forward, x, n, axis, norm, overwrite_x, workers, plan)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid number of data points (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtmp\u001b[38;5;241m.\u001b[39mshape[axis]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) specified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Note: overwrite_x is not utilised\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpfft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mr2c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TimeCNN(num_classes=6).to(device)\n",
    "            \n",
    "# model summary\n",
    "stats = summary(model, input_size=(49, 216))\n",
    "\n",
    "# Train\n",
    "best_acc, history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=validation_loader,\n",
    "    epochs=50,\n",
    "    lr=1e-4,\n",
    "    patience=10,\n",
    "    device=device,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "print(f\"Best Accuracy: {best_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
